{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLT 2022 MNIST Replication\n",
    "\n",
    "This notebook replicates the COLT 2022 MNIST feature extractor comparison in the new `pyac` codebase, including the split assembly result at `m=10,000`.\n",
    "\n",
    "Compared to the old `brain.py` + `MNIST_original.ipynb` flow, this notebook uses explicit RNG threading, modular extractors, and full MNIST (60k/10k) evaluation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pyac.core.rng import make_rng\n",
    "from pyac.tasks.mnist.extractors import (\n",
    "    LinearExtractor,\n",
    "    NonlinearExtractor,\n",
    "    LargeAreaExtractor,\n",
    "    RandomAssemblyExtractor,\n",
    "    SplitAssemblyExtractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full MNIST dataset\n",
    "try:\n",
    "    X, y = fetch_openml('mnist_784', version=1, parser='auto', as_frame=False, return_X_y=True)\n",
    "    X = np.asarray(X, dtype=np.float64) / 255.0  # Normalize to [0,1]\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "except Exception:\n",
    "    # Fallback when OpenML endpoint or parser requirements are unavailable\n",
    "    import io\n",
    "    import urllib.request\n",
    "\n",
    "    url = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
    "    with urllib.request.urlopen(url, timeout=60) as response:\n",
    "        payload = response.read()\n",
    "    with np.load(io.BytesIO(payload)) as data:\n",
    "        X_train_raw = data['x_train']\n",
    "        y_train_raw = data['y_train']\n",
    "        X_test_raw = data['x_test']\n",
    "        y_test_raw = data['y_test']\n",
    "\n",
    "    X = np.concatenate([X_train_raw, X_test_raw], axis=0).reshape(70000, 784).astype(np.float64) / 255.0\n",
    "    y = np.concatenate([y_train_raw, y_test_raw], axis=0).astype(np.int64)\n",
    "\n",
    "# Split train/test\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_values = [100, 500, 1000, 5000, 10000]\n",
    "n_seeds = 3  # Use 3 seeds for faster execution (paper used 5)\n",
    "extractors = {\n",
    "    'linear': lambda m: LinearExtractor(m=m),\n",
    "    'nonlinear': lambda m: NonlinearExtractor(m=m),\n",
    "    'large_area': lambda m: LargeAreaExtractor(m=m, beta=1.0, t_internal=5, n_examples_per_class=5),\n",
    "    'random_assembly': lambda m: RandomAssemblyExtractor(m=m, beta=1.0, t_internal=5, n_examples_per_class=5) if m % 100 == 0 else None,\n",
    "    'split_assembly': lambda m: SplitAssemblyExtractor(m=m, beta=1.0, t_internal=5, n_examples_per_class=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "images_train = [img for img in X_train]\n",
    "images_test = [img for img in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for extractor_name, extractor_fn in extractors.items():\n",
    "    print(f\"\\n=== {extractor_name.upper()} ===\")\n",
    "    for m in m_values:\n",
    "        # Skip random_assembly for m not multiple of 100\n",
    "        ext = extractor_fn(m)\n",
    "        if ext is None:\n",
    "            print(f\"  m={m:5d}: SKIPPED (not multiple of 100)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  m={m:5d}: \", end='', flush=True)\n",
    "\n",
    "        for seed in range(n_seeds):\n",
    "            rng = make_rng(seed)\n",
    "\n",
    "            # Fit extractor\n",
    "            ext.fit(images_train, y_train, rng)\n",
    "\n",
    "            # Transform\n",
    "            features_train = ext.transform(images_train, rng)\n",
    "            features_test = ext.transform(images_test, rng)\n",
    "\n",
    "            # Train LogisticRegression\n",
    "            clf = LogisticRegression(max_iter=1000, random_state=seed)\n",
    "            clf.fit(features_train, y_train)\n",
    "            accuracy = clf.score(features_test, y_test)\n",
    "\n",
    "            results.append({\n",
    "                'extractor': extractor_name,\n",
    "                'n_features': m,\n",
    "                'seed': seed,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "\n",
    "            print(f\"{accuracy:.1%} \", end='', flush=True)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "agg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve repository root for stable artifact paths\n",
    "repo_root = Path.cwd()\n",
    "while not (repo_root / '.git').exists() and repo_root != repo_root.parent:\n",
    "    repo_root = repo_root.parent\n",
    "output_dir = repo_root / 'pyac' / 'notebooks' / 'replication'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Completed {len(results)} runs across extractors and feature counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean accuracy per extractor per m\n",
    "grouped = {}\n",
    "for row in results:\n",
    "    key = (row['extractor'], row['n_features'])\n",
    "    grouped.setdefault(key, []).append(row['accuracy'])\n",
    "\n",
    "agg = [\n",
    "    {'extractor': key[0], 'n_features': key[1], 'accuracy': float(np.mean(values))}\n",
    "    for key, values in grouped.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "for extractor_name in ['linear', 'nonlinear', 'large_area', 'random_assembly', 'split_assembly']:\n",
    "    subset = sorted(\n",
    "        [row for row in agg if row['extractor'] == extractor_name],\n",
    "        key=lambda row: row['n_features']\n",
    "    )\n",
    "    if len(subset) > 0:\n",
    "        x = [row['n_features'] for row in subset]\n",
    "        y = [row['accuracy'] for row in subset]\n",
    "        plt.plot(x, y, 'o-', label=extractor_name, linewidth=2, markersize=8)\n",
    "\n",
    "plt.xlabel('Number of Features (m)', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('COLT 2022 Replication: MNIST Accuracy vs Feature Count', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "\n",
    "# Save plot\n",
    "plot_path = output_dir / 'accuracy_vs_features.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved plot: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Split assembly at m=10,000: XX.X%\n",
    "- Target: >=90% (paper shows ~96%)\n",
    "- Status: PASS/FAIL\n",
    "\n",
    "Top 3 extractors at m=10,000:\n",
    "1. split_assembly: XX.X%\n",
    "2. random_assembly: XX.X%\n",
    "3. large_area: XX.X%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Old Codebase\n",
    "\n",
    "**Old brain.py MNIST results** (from MNIST_original.ipynb):\n",
    "- Used dense numpy arrays\n",
    "- RefractedArea with negative bias\n",
    "- Achieved ~68% accuracy in demo\n",
    "\n",
    "**New pyac results**:\n",
    "- Sparse CSR/CSC matrices (G1)\n",
    "- RefractedStrategy with beta plasticity\n",
    "- Split assembly achieves ~XX% at 10k features\n",
    "\n",
    "**Key improvements**:\n",
    "- 10x-100x memory savings with sparse matrices\n",
    "- Explicit RNG threading for reproducibility\n",
    "- Pluggable feature extractors (5 variants)\n",
    "- Full MNIST (60k train) vs subset in old demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics.json\n",
    "metrics = {\n",
    "    'results': results,\n",
    "    'parameters': {\n",
    "        'm_values': m_values,\n",
    "        'n_seeds': n_seeds,\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_path = output_dir / 'metrics.json'\n",
    "with metrics_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Saved metrics.json: {metrics_path}\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'extractors': list(extractors.keys()),\n",
    "    'feature_counts': m_values,\n",
    "    'seeds': list(range(n_seeds)),\n",
    "    'dataset': 'mnist_784',\n",
    "    'classifier': 'LogisticRegression',\n",
    "    'max_iter': 1000\n",
    "}\n",
    "\n",
    "config_path = output_dir / 'replication_config.json'\n",
    "with config_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Saved replication_config.json: {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
